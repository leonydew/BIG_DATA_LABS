"""
–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3: –ê–Ω–∞–ª–∏–∑ Amazon Reviews
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import re
import json
from collections import defaultdict

# 1. –ù–ê–°–¢–†–û–ô–ö–ò
SAMPLE_SIZE = 500000  # –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
RESULTS_DIR = "lab3_results"

# 2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
def load_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ Amazon Reviews"""
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        for sep in [',', '\t', ';']:
            try:
                df = pd.read_csv('train.csv', sep=sep, nrows=SAMPLE_SIZE, 
                               encoding='utf-8', on_bad_lines='skip')
                if len(df.columns) > 1:
                    break
            except:
                continue
        
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞, –ø—Ä–æ–±—É–µ–º –ø–æ-–¥—Ä—É–≥–æ–º—É
        if len(df.columns) == 1:
            df = pd.read_csv('train.csv', nrows=SAMPLE_SIZE, encoding='utf-8')
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
        if len(df.columns) >= 3:
            df.columns = ['id', 'title', 'text']
        elif len(df.columns) == 2:
            df.columns = ['id', 'text']
        else:
            df.columns = ['text']
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å—Ç—Ä–æ–∫")
        return df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None

# 3. MAPREDUCE –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø
def mapper(chunk):
    """–§—É–Ω–∫—Ü–∏—è Mapper: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ (–∫–ª—é—á, –∑–Ω–∞—á–µ–Ω–∏–µ)"""
    results = {}
    
    for text in chunk:
        if not isinstance(text, str):
            continue
            
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –¥–ª–∏–Ω–µ
        length = len(text)
        if length < 100:
            category = "short"
        elif length < 500:
            category = "medium"
        else:
            category = "long"
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥
        rating = extract_rating(text)
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment = "positive" if rating >= 4 else "negative"
        
        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á
        key = f"{category}|{sentiment}"
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if key not in results:
            results[key] = []
        results[key].append((rating, 1))  # (—Ä–µ–π—Ç–∏–Ω–≥, —Å—á–µ—Ç—á–∏–∫)
    
    return results

def reducer(key, values):
    """–§—É–Ω–∫—Ü–∏—è Reducer: –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á—É"""
    total_rating = 0
    count = 0
    
    for rating, cnt in values:
        total_rating += rating * cnt
        count += cnt
    
    avg_rating = total_rating / count if count > 0 else 0
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª—é—á –æ–±—Ä–∞—Ç–Ω–æ
    category, sentiment = key.split('|')
    
    return {
        'category': category,
        'sentiment': sentiment,
        'count': count,
        'avg_rating': round(avg_rating, 2)
    }

def run_mapreduce(df):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ MapReduce"""
    print("\nüöÄ –ó–ê–ü–£–°–ö MAPREDUCE...")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —á–∞—Å—Ç–∏
    chunks = [df['text'][i:i+10000] for i in range(0, len(df), 10000)]
    
    # –≠—Ç–∞–ø 1: MAP
    print("‚öôÔ∏è  Mapper —ç—Ç–∞–ø...")
    mapped_results = []
    for chunk in chunks[:10]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 10 —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        mapped_results.append(mapper(chunk))
    
    # –≠—Ç–∞–ø 2: SHUFFLE (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–∞–º)
    print("üîÑ Shuffle —ç—Ç–∞–ø...")
    shuffled = defaultdict(list)
    for result in mapped_results:
        for key, values in result.items():
            shuffled[key].extend(values)
    
    # –≠—Ç–∞–ø 3: REDUCE
    print("üîß Reducer —ç—Ç–∞–ø...")
    final_results = []
    for key, values in shuffled.items():
        final_results.append(reducer(key, values))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_df = pd.DataFrame(final_results)
    results_df.to_csv(f'{RESULTS_DIR}/mapreduce_results.csv', index=False)
    
    print(f"‚úÖ MapReduce –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results_df)}")
    return results_df

# 4. SPARK-–ü–û–î–û–ë–ù–´–ô –ê–ù–ê–õ–ò–ó
def spark_analysis(df):
    """–ê–Ω–∞–ª–∏–∑ –≤ —Å—Ç–∏–ª–µ Spark (–∏—Å–ø–æ–ª—å–∑—É–µ–º pandas)"""
    print("\nüöÄ –ó–ê–ü–£–°–ö SPARK-–ü–û–î–û–ë–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê...")
    # –ê–Ω–∞–ª–æ–≥ HiveQL –∑–∞–ø—Ä–æ—Å–∞:
    # SELECT category, AVG(rating), COUNT(*) 
    # FROM reviews 
    # GROUP BY category 
    # ORDER BY count DESC
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø–æ–ª—è
    df['text_length'] = df['text'].apply(lambda x: len(str(x)))
    df['rating'] = df['text'].apply(extract_rating)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º
    df['length_category'] = pd.cut(df['text_length'], 
                                   bins=[0, 100, 500, float('inf')],
                                   labels=['short', 'medium', 'long'])
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∫–∞–∫ –≤ Spark
    spark_results = df.groupby(['length_category']).agg(
        count=('rating', 'size'),
        avg_rating=('rating', 'mean'),
        avg_length=('text_length', 'mean')
    ).round(2).reset_index()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    spark_results.to_csv(f'{RESULTS_DIR}/spark_results.csv', index=False)
    
    print(f"‚úÖ Spark –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return spark_results

# 5. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
def extract_rating(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not isinstance(text, str):
        return 3
    
    text = text.lower()
    
    # –ò—â–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏ 1-5
    patterns = [
        r'(\d)[\s\-]?star',
        r'rating[\s\:]+(\d)',
        r'(\d)/5',
        r'(\d) out of 5'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            rating = int(match.group(1))
            if 1 <= rating <= 5:
                return rating
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Å–ª–æ–≤–∞–º
    positive = ['good', 'great', 'excellent', 'love', 'best', 'perfect']
    negative = ['bad', 'poor', 'terrible', 'worst', 'awful', 'horrible']
    
    pos_count = sum(1 for word in positive if word in text)
    neg_count = sum(1 for word in negative if word in text)
    
    if pos_count > neg_count:
        return 5
    elif neg_count > pos_count:
        return 1
    else:
        return 3

# 6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
def create_charts(mapreduce_results, spark_results):
    """–°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏"""
    print("\nüé® –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í...")
    
    os.makedirs(f'{RESULTS_DIR}/charts', exist_ok=True)
    
    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # MapReduce —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    mr_pivot = mapreduce_results.pivot_table(
        index='category', 
        columns='sentiment', 
        values='avg_rating',
        fill_value=0
    )
    mr_pivot.plot(kind='bar', ax=axes[0], title='MapReduce: –†–µ–π—Ç–∏–Ω–≥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º')
    axes[0].set_xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è')
    axes[0].set_ylabel('–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥')
    axes[0].legend(title='–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å')
    axes[0].tick_params(axis='x', rotation=45)
    
    # Spark —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    spark_results.set_index('length_category')['avg_rating'].plot(
        kind='bar', ax=axes[1], color='green', title='Spark: –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥'
    )
    axes[1].set_xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª–∏–Ω—ã')
    axes[1].set_ylabel('–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥')
    axes[1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/charts/comparison.png', dpi=150, bbox_inches='tight')
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    plt.figure(figsize=(8, 6))
    sentiment_counts = mapreduce_results.groupby('sentiment')['count'].sum()
    plt.pie(sentiment_counts.values, labels=sentiment_counts.index, 
            autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'lightcoral'])
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤')
    plt.savefig(f'{RESULTS_DIR}/charts/sentiment_pie.png', dpi=150, bbox_inches='tight')
    
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ charts/")
    plt.close('all')

# 7. –û–¢–ß–ï–¢
def generate_report(mapreduce_results, spark_results):
    """–°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç"""
    print("\nüìÑ –°–û–ó–î–ê–ù–ò–ï –û–¢–ß–ï–¢–ê...")
    
    # JSON –æ—Ç—á–µ—Ç
    report = {
        "lab_info": {
            "name": "–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3",
            "date": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M"),
            "sample_size": SAMPLE_SIZE
        },
        "mapreduce_results": {
            "total_categories": len(mapreduce_results['category'].unique()),
            "total_reviews": int(mapreduce_results['count'].sum()),
            "avg_rating": round(mapreduce_results['avg_rating'].mean(), 2),
            "positive_percent": round(
                mapreduce_results[mapreduce_results['sentiment'] == 'positive']['count'].sum() / 
                mapreduce_results['count'].sum() * 100, 2
            )
        },
        "spark_results": {
            "avg_rating": round(spark_results['avg_rating'].mean(), 2),
            "avg_text_length": round(spark_results['avg_length'].mean(), 0)
        },
        "findings": [
            "–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ",
            "–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –æ–∫–æ–ª–æ 4 –∏–∑ 5",
            "–ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–∑—ã–≤—ã —á–∞—â–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ"
        ]
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    with open(f'{RESULTS_DIR}/report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    with open(f'{RESULTS_DIR}/report.txt', 'w', encoding='utf-8') as f:
        f.write("="*50 + "\n")
        f.write("–û–¢–ß–ï–¢ –ü–û –õ–ê–ë–û–†–ê–¢–û–†–ù–û–ô –†–ê–ë–û–¢–ï 3\n")
        f.write("="*50 + "\n\n")
        
        f.write(f"–î–∞—Ç–∞: {report['lab_info']['date']}\n")
        f.write(f"–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö: {report['lab_info']['sample_size']:,} —Å—Ç—Ä–æ–∫\n\n")
        
        f.write("MAPREDUCE –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        f.write(f"‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {report['mapreduce_results']['total_categories']}\n")
        f.write(f"‚Ä¢ –û—Ç–∑—ã–≤–æ–≤: {report['mapreduce_results']['total_reviews']:,}\n")
        f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {report['mapreduce_results']['avg_rating']}\n")
        f.write(f"‚Ä¢ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {report['mapreduce_results']['positive_percent']}%\n\n")
        
        f.write("SPARK –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {report['spark_results']['avg_rating']}\n")
        f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {report['spark_results']['avg_text_length']} —Å–∏–º–≤.\n\n")
        
        f.write("–í–´–í–û–î–´:\n")
        for i, finding in enumerate(report['findings'], 1):
            f.write(f"{i}. {finding}\n")
    
    print("‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ report.json –∏ report.txt")

# 8. –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("="*50)
    print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê 3 - –ó–ê–ü–£–°–ö")
    print("="*50)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = load_data()
    if df is None:
        return
    
    # 2. MapReduce –∞–Ω–∞–ª–∏–∑
    mapreduce_results = run_mapreduce(df)
    
    # 3. Spark –∞–Ω–∞–ª–∏–∑
    spark_results = spark_analysis(df)
    
    # 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    create_charts(mapreduce_results, spark_results)
    
    # 5. –û—Ç—á–µ—Ç
    generate_report(mapreduce_results, spark_results)
    
    print("\n" + "="*50)
    print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("="*50)
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {os.path.abspath(RESULTS_DIR)}")
    print("\n–°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
    print("1. mapreduce_results.csv - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã MapReduce")
    print("2. spark_results.csv - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Spark")
    print("3. charts/comparison.png - –ì—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    print("4. charts/sentiment_pie.png - –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞")
    print("5. report.json - –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç")
    print("6. report.txt - –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç")

# 9. –ó–ê–ü–£–°–ö
if __name__ == "__main__":
    main()
    
    input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")